{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lesson3_deeplearning_crop_segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python [conda env:servir]",
      "language": "python",
      "name": "conda-env-servir-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYe-1iFbo3dQ"
      },
      "source": [
        "# Semantic segmentation with deep learning\n",
        "> A guide for using deep-learning based semantic segmentation to land use / land cover in satellite imagery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYmAYTogOkpd"
      },
      "source": [
        "In this tutorial we will learn how to segment images according to a set of classes. **Segmentation**  refers to the process of partitioning an image into groups of pixels that identify with a target class within the foreground or the background class (a catch-all class that contains non-target features).\n",
        "\n",
        "Specifically, in this tutorial we will be using data from two reference datasets, Imaflora and Para, depicting land use classifications of relevant areas in the Amazon. \n",
        "\n",
        "We will pair the reference data with a Planet 5 meter composite made from data available as part of the NICFI program. Our task will be to predict the land use / land cover in an image on a pixel-wise basis. \n",
        "\n",
        "## Specific concepts that will be covered\n",
        "In the process, we will build practical experience and develop intuition around the following concepts:\n",
        "* **[Functional API](https://keras.io/getting-started/functional-api-guide/)** - we will be implementing UNet, a convolutional network model classically used for biomedical image segmentation with the Functional API. \n",
        "  * This model has layers that require multiple input/outputs. This requires the use of the functional API\n",
        "  * Check out the original [paper](https://arxiv.org/abs/1505.04597), \n",
        "U-Net: Convolutional Networks for Biomedical Image Segmentation by Olaf Ronneberger!\n",
        "* **Loss Functions and Metrics** - We'll implement the **Sparse Categorical [focal loss](https://focal-loss.readthedocs.io/en/latest/) function**\n",
        " and **accuracy**. We'll also generate confusion matrices during evaluation to judge how well the model performs. \n",
        "* **Saving and loading Keras models** - We'll save our best model to file. When we want to perform inference/evaluate our model in the future, we can load the model file. \n",
        "\n",
        "### General Workflow\n",
        "1. Load image and label datasets from Google Drive\n",
        "2. Visualize data/perform some exploratory data analysis\n",
        "3. Set up data pipeline and preprocessing\n",
        "4. Build model\n",
        "5. Train model\n",
        "6. Test model\n",
        "\n",
        "### Objectives\n",
        "1. Practice with the Keras Functional API as a means to run TensorFlow models\n",
        "2. Experience training a segmentation model and monitoring progress\n",
        "3. Learn how to generate predictions with a trained segmentation model\n",
        "\n",
        "\n",
        "**Audience:** This post is geared towards intermediate users who are comfortable with basic machine learning concepts. \n",
        "\n",
        "**Time Estimated**: 60-120 min\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wLP8n9tvVwD"
      },
      "source": [
        "## Setup Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7KESWOyQsvB"
      },
      "source": [
        "```{admonition} **Version control**\n",
        "Colab updates without warning to users, which can cause notebooks to break. Therefore, we are pinning library versions.\n",
        "``` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvF43tyLcWd7"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZziz8vAclU6"
      },
      "source": [
        "!pip --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLzR_E3EcoS6"
      },
      "source": [
        "!pip install pip==21.1.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYMOkBeaeuoV"
      },
      "source": [
        "# install required libraries\n",
        "!pip install -q rasterio==1.2.10\n",
        "!pip install -q geopandas==0.10.2\n",
        "!pip install -q git+https://github.com/tensorflow/examples.git\n",
        "!pip install -q -U tfds-nightly\n",
        "!pip install -q focal-loss\n",
        "!pip install -q tensorflow-addons==0.8.3\n",
        "#!pip install -q matplotlib==3.5 # UNCOMMENT if running on LOCAL\n",
        "!pip install -q scikit-learn==1.0.1\n",
        "!pip install -q scikit-image==0.18.3\n",
        "!pip install -q tf-explain==0.3.1\n",
        "!pip install -q segmentation_models # we'll use this for pretraining later and for the IOU segmentation performance metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMn0p6YLK5A4"
      },
      "source": [
        "# import required libraries\n",
        "import os, glob, functools, fnmatch, io, shutil\n",
        "from zipfile import ZipFile\n",
        "from itertools import product\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "mpl.rcParams['figure.figsize'] = (12,12)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import rasterio\n",
        "from rasterio import features, mask\n",
        "\n",
        "import geopandas as gpd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import layers, losses, models\n",
        "from tensorflow.python.keras import backend as K  \n",
        "import tensorflow_addons as tfa\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "from segmentation_models.metrics import iou_score\n",
        "from tf_explain.callbacks.activations_visualization import ActivationsVisualizationCallback\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "from tqdm.notebook import tqdm\n",
        "import datetime\n",
        "\n",
        "from focal_loss import SparseCategoricalFocalLoss\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import skimage.io as skio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA43wKIZK5A5"
      },
      "source": [
        "#### Getting set up with the data\n",
        "\n",
        "```{important}\n",
        "Create drive shortcuts of the tiled imagery to your own My Drive Folder by Right-Clicking on the Shared folder `servir-tf-devseed`. Then, this folder will be available at the following path that is accessible with the google.colab `drive` module: `'/content/gdrive/My Drive/servir-tf-devseed/'`\n",
        "```\n",
        "\n",
        "We'll be working witht he following folders in the `servir-tf-devseed` folder:\n",
        "```\n",
        "servir-tf-devseed/\n",
        "├── images/\n",
        "├── images_bright/\n",
        "├── indices/\n",
        "├── indices_800/\n",
        "├── labels/\n",
        "├── labels_800/\n",
        "├── background_list_train.txt\n",
        "├── train_list_clean.txt\n",
        "└── terrabio_classes.csv\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_UgojhBOkpe"
      },
      "source": [
        "# set your root directory and tiled data folders\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    # this is a google colab specific command to ensure TF version 2 is used. \n",
        "    # it won't work in a regular jupyter notebook, for a regular notebook make sure you install TF version 2\n",
        "    %tensorflow_version 2.x\n",
        "    # mount google drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    root_dir = '/content/gdrive/My Drive/servir-tf-devseed/' \n",
        "    workshop_dir = '/content/gdrive/My Drive/servir-tf-devseed-workshop'\n",
        "    print('Running on Colab')\n",
        "else:\n",
        "    root_dir = os.path.abspath(\"./data/servir-tf-devseed\")\n",
        "    workshop_dir = os.path.abspath('./servir-tf-devseed-workshop')\n",
        "    print(f'Not running on Colab, data needs to be downloaded locally at {os.path.abspath(root_dir)}')\n",
        "\n",
        "img_dir = os.path.join(root_dir,'indices/') # or os.path.join(root_dir,'images_bright/') if using the optical tiles\n",
        "label_dir = os.path.join(root_dir,'labels/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEmJkl8AlE1r"
      },
      "source": [
        "# go to root directory\n",
        "%cd $root_dir "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmxfUyLrvheG"
      },
      "source": [
        "### Enabling GPU\n",
        "\n",
        "This notebook can utilize a GPU and works better if you use one. Hopefully this notebook is using a GPU, and we can check with the following code.\n",
        "\n",
        "If it's not using a GPU you can change your session/notebook to use a GPU. See [Instructions](https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=sXnDmXR7RDr2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2-WXVFfvi1S"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldG5RFPFNgOo"
      },
      "source": [
        "### Check out the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAbEMB8i87FL"
      },
      "source": [
        "# Read the classes\n",
        "class_index = pd.read_csv(os.path.join(root_dir,'terrabio_classes.csv'))\n",
        "class_names = class_index.class_name.unique()\n",
        "print(class_index) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI4907uiOkqN"
      },
      "source": [
        "### Read into tensorflow datasets\n",
        "\n",
        "Now we will compile the spectral index image and label tiles into training, validation, and test datasets for use with TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSml5T3kQNKK"
      },
      "source": [
        "```{warning} **Long running cells** \\\n",
        "Normally we would read the image files from the directories and then process forward from there with background removal with the next **three** illustrated functions, however, due to slow I/O in Google Colab we will read the images list with 90% background removal already performed from a pre-saved list in the shared drive.\n",
        "```\n",
        "\n",
        "```python\n",
        "# get lists of image and label tile pairs for training and testing\n",
        "\n",
        "def get_train_test_lists(imdir, lbldir):\n",
        "  imgs = glob.glob(os.path.join(imdir,\"/*.png\"))\n",
        "  #print(imgs[0:1])\n",
        "  dset_list = []\n",
        "  for img in imgs:\n",
        "    filename_split = os.path.splitext(img) \n",
        "    filename_zero, fileext = filename_split \n",
        "    basename = os.path.basename(filename_zero) \n",
        "    dset_list.append(basename)\n",
        "    \n",
        "  x_filenames = []\n",
        "  y_filenames = []\n",
        "  for img_id in dset_list:\n",
        "    x_filenames.append(os.path.join(imdir, \"{}.png\".format(img_id)))\n",
        "    y_filenames.append(os.path.join(lbldir, \"{}.png\".format(img_id)))\n",
        "    \n",
        "  print(\"number of images: \", len(dset_list))\n",
        "  return dset_list, x_filenames, y_filenames\n",
        "\n",
        "train_list, x_train_filenames, y_train_filenames = get_train_test_lists(img_dir, label_dir)\n",
        "\n",
        "```\n",
        "number of images:  37350\n",
        "\n",
        "Check for the proportion of background tiles. This takes a while. So we can skip by loading from saved results.\n",
        "\n",
        "```python\n",
        "skip = True\n",
        "\n",
        "if not skip:\n",
        "  background_list_train = []\n",
        "  for i in train_list: \n",
        "      # read in each labeled images\n",
        "      # print(os.path.join(label_dir,\"{}.png\".format(i))) \n",
        "      img = np.array(Image.open(os.path.join(label_dir,\"{}.png\".format(i))))  \n",
        "      # check if no values in image are greater than zero (background value)\n",
        "      if img.max()==0:\n",
        "          background_list_train.append(i)\n",
        "          \n",
        "  print(\"Number of background images: \", len(background_list_train))\n",
        "\n",
        "  with open(os.path.join(root_dir,'background_list_train.txt'), 'w') as f:\n",
        "    for item in background_list_train:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "else:\n",
        "  background_list_train = [line.strip() for line in open(\"background_list_train.txt\", 'r')]\n",
        "  print(\"Number of background images: \", len(background_list_train))\n",
        "```\n",
        "Number of background images:  36489\n",
        "\n",
        "We will keep only 10% of the total. Too many background tiles can cause a form of class imbalance.\n",
        "```python\n",
        "background_removal = len(background_list_train) * 0.9\n",
        "train_list_clean = [y for y in train_list if y not in background_list_train[0:int(background_removal)]]\n",
        "\n",
        "x_train_filenames = []\n",
        "y_train_filenames = []\n",
        "\n",
        "for i, img_id in zip(tqdm(range(len(train_list_clean))), train_list_clean):\n",
        "  pass \n",
        "  x_train_filenames.append(os.path.join(img_dir, \"{}.png\".format(img_id)))\n",
        "  y_train_filenames.append(os.path.join(label_dir, \"{}.png\".format(img_id)))\n",
        "\n",
        "print(\"Number of background tiles: \", background_removal)\n",
        "print(\"Remaining number of tiles after 90% background removal: \", len(train_list_clean))\n",
        "```\n",
        "Number of background tiles:  32840\n",
        "\n",
        "Remaining number of tiles after 90% background removal:  4510\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cbWXhA8S1fI"
      },
      "source": [
        "```{important}\n",
        "The cell below contains the shortcut read of prepped training image list. \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfcEB0j-PC0R"
      },
      "source": [
        "  def get_train_test_lists(imdir, lbldir):\n",
        "    train_list = [line.strip() for line in open(\"train_list_clean.txt\", 'r')]\n",
        "\n",
        "    x_filenames = []\n",
        "    y_filenames = []\n",
        "    for img_id in train_list:\n",
        "      x_filenames.append(os.path.join(imdir, \"{}.png\".format(img_id)))\n",
        "      y_filenames.append(os.path.join(lbldir, \"{}.png\".format(img_id)))\n",
        "\n",
        "    print(\"Number of images: \", len(train_list))\n",
        "    return train_list, x_filenames, y_filenames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb8XzdHIP_Df"
      },
      "source": [
        "train_list, x_train_filenames, y_train_filenames = get_train_test_lists(img_dir, label_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jr4gLtGOkqR"
      },
      "source": [
        "Now that we have our set of files we want to use for developing our model, we need to split them into three sets: \n",
        "* the training set for the model to learn from\n",
        "* the validation set that allows us to evaluate models and make decisions to change models\n",
        "* and the test set that we will use to communicate the results of the best performing model (as determined by the validation set)\n",
        "\n",
        "We will split index tiles and label tiles into train, validation and test sets: 70%, 20% and 10%, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3tOcxtwOkqU"
      },
      "source": [
        "x_train_filenames, x_val_filenames, y_train_filenames, y_val_filenames = train_test_split(x_train_filenames, y_train_filenames, test_size=0.3, random_state=42)\n",
        "x_val_filenames, x_test_filenames, y_val_filenames, y_test_filenames = train_test_split(x_val_filenames, y_val_filenames, test_size=0.33, random_state=42)\n",
        "\n",
        "num_train_examples = len(x_train_filenames)\n",
        "num_val_examples = len(x_val_filenames)\n",
        "num_test_examples = len(x_test_filenames)\n",
        "\n",
        "print(\"Number of training examples: {}\".format(num_train_examples))\n",
        "print(\"Number of validation examples: {}\".format(num_val_examples))\n",
        "print(\"Number of test examples: {}\".format(num_test_examples))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI5pctxn6c0Z"
      },
      "source": [
        "```{warning} **Long running cell** \\\n",
        "The code below checks for values in train, val, and test partitions. We won't run this since it takes over 10 minutes on colab due to slow I/O.\n",
        "```\n",
        "```python\n",
        "vals_train = []\n",
        "vals_val = []\n",
        "vals_test = []\n",
        "\n",
        "def get_vals_in_partition(partition_list, x_filenames, y_filenames):\n",
        "  for x,y,i in zip(x_filenames, y_filenames, tqdm(range(len(y_filenames)))):\n",
        "      pass \n",
        "      try:\n",
        "        img = np.array(Image.open(y)) \n",
        "        vals = np.unique(img)\n",
        "        partition_list.append(vals)\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "def flatten(partition_list):\n",
        "    return [item for sublist in partition_list for item in sublist]\n",
        "\n",
        "get_vals_in_partition(vals_train, x_train_filenames, y_train_filenames)\n",
        "get_vals_in_partition(vals_val, x_val_filenames, y_val_filenames)\n",
        "get_vals_in_partition(vals_test, x_test_filenames, y_test_filenames)\n",
        "\n",
        "print(\"Values in training partition: \", set(flatten(vals_train)))\n",
        "print(\"Values in validation partition: \", set(flatten(vals_val)))\n",
        "print(\"Values in test partition: \", set(flatten(vals_test)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkT8aQf0jwxT",
        "outputId": "c3e7500b-b047-4962-df07-35aa62304ae3"
      },
      "source": [
        "\n",
        "Values in training partition:  {0, 1, 2, 3, 4, 5, 6, 7, 8}\n",
        "\n",
        "Values in validation partition:  {0, 1, 2, 3, 4, 5, 6, 7, 8}\n",
        "\n",
        "Values in test partition:  {0, 1, 2, 3, 4, 5, 6, 7, 8}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRfSZbtUOkqW"
      },
      "source": [
        "### Visualize the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NArAfH9WOLLZ"
      },
      "source": [
        "```{warning} **Long running cell** \\\n",
        "The code below loads foreground examples randomly. We won't run this since it takes over a while on colab due to slow I/O.\n",
        "```\n",
        "<div>&#8681</div> \n",
        "\n",
        "```python\n",
        "display_num = 3\n",
        "\n",
        "background_list_train = [line.strip() for line in open(\"background_list_train.txt\", 'r')]\n",
        "\n",
        "# select only for tiles with foreground labels present\n",
        "foreground_list_x = []\n",
        "foreground_list_y = []\n",
        "for x,y in zip(x_train_filenames, y_train_filenames): \n",
        "    try:\n",
        "      filename_split = os.path.splitext(y) \n",
        "      filename_zero, fileext = filename_split \n",
        "      basename = os.path.basename(filename_zero) \n",
        "      if basename not in background_list_train:\n",
        "        foreground_list_x.append(x)\n",
        "        foreground_list_y.append(y)\n",
        "      else:\n",
        "        continue\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "num_foreground_examples = len(foreground_list_y)\n",
        "\n",
        "# randomlize the choice of image and label pairs\n",
        "r_choices = np.random.choice(num_foreground_examples, display_num)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKt4naGxO1Yz"
      },
      "source": [
        "```{important}\n",
        "Instead, we will read and plot a few sample foreground training images and labels from their pathnames. Note: this may still take a few execution tries to work. Google colab in practice takes some time to connect to data in Google Drive, so sometimes this returns an error on the first (few) attempt(s).\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVTOh8gIOkqW"
      },
      "source": [
        "display_num = 3\n",
        "\n",
        "background_list_train = [line.strip() for line in open(\"background_list_train.txt\", 'r')]\n",
        "\n",
        "foreground_list_x = [\n",
        "                     f'{img_dir}/tile_terrabio_15684.png', \n",
        "                     f'{img_dir}/tile_terrabio_23056.png', \n",
        "                     f'{img_dir}/tile_terrabio_21877.png'\n",
        "                     ]\n",
        "\n",
        "foreground_list_y = [\n",
        "                     f'{label_dir}/tile_terrabio_15684.png', \n",
        "                     f'{label_dir}/tile_terrabio_23056.png', \n",
        "                     f'{label_dir}/tile_terrabio_21877.png'\n",
        "                     ]\n",
        "\n",
        "# confirm files exist\n",
        "for fx, fy in zip(foreground_list_x, foreground_list_y):\n",
        "  if os.path.isfile(fx) and os.path.isfile(fy):\n",
        "    print(fx, \" and \", fy, \" exist.\")\n",
        "  else:\n",
        "    print(fx, \" and \", fy, \" don't exist.\")\n",
        "\n",
        "num_foreground_examples = len(foreground_list_y)\n",
        "\n",
        "# randomlize the choice of image and label pairs\n",
        "#r_choices = np.random.choice(num_foreground_examples, display_num)\n",
        "\n",
        "plt.figure(figsize=(10, 15))\n",
        "for i in range(0, display_num * 2, 2):\n",
        "  #img_num = r_choices[i // 2]\n",
        "  img_num = i // 2\n",
        "  x_pathname = foreground_list_x[img_num]\n",
        "  y_pathname = foreground_list_y[img_num]\n",
        "  \n",
        "  plt.subplot(display_num, 2, i + 1)\n",
        "  plt.imshow(mpimg.imread(x_pathname))\n",
        "  plt.title(\"Original Image\")\n",
        "  \n",
        "  example_labels = Image.open(y_pathname)\n",
        "  label_vals = np.unique(np.array(example_labels))\n",
        "  \n",
        "  plt.subplot(display_num, 2, i + 2)\n",
        "  plt.imshow(example_labels)\n",
        "  plt.title(\"Masked Image\")  \n",
        "  \n",
        "plt.suptitle(\"Examples of Images and their Masks\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flAQ44h_OkqX"
      },
      "source": [
        "### Read the tiles into tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR6kpJZrOkqY"
      },
      "source": [
        "# set input image shape\n",
        "img_shape = (224, 224, 3)\n",
        "# set batch size for model\n",
        "batch_size = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SY1rECtK5A-"
      },
      "source": [
        "# Function for reading the tiles into TensorFlow tensors \n",
        "# See TensorFlow documentation for explanation of tensor: https://www.tensorflow.org/guide/tensor\n",
        "def _process_pathnames(fname, label_path):\n",
        "  # We map this function onto each pathname pair  \n",
        "  img_str = tf.io.read_file(fname)\n",
        "  img = tf.image.decode_png(img_str, channels=3)\n",
        "\n",
        "  label_img_str = tf.io.read_file(label_path)\n",
        "\n",
        "  # These are png images so they return as (num_frames, h, w, c)\n",
        "  label_img = tf.image.decode_png(label_img_str, channels=1)\n",
        "  # The label image should have any values between 0 and 8, indicating pixel wise\n",
        "  # foreground class or background (0). We take the first channel only. \n",
        "  label_img = label_img[:, :, 0]\n",
        "  label_img = tf.expand_dims(label_img, axis=-1)\n",
        "  return img, label_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrPwcncnK5A-"
      },
      "source": [
        "# Function to augment the data with horizontal flip\n",
        "def flip_img_h(horizontal_flip, tr_img, label_img):\n",
        "  if horizontal_flip:\n",
        "    flip_prob = tf.random.uniform([], 0.0, 1.0)\n",
        "    tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n",
        "                                lambda: (tf.image.flip_left_right(tr_img), tf.image.flip_left_right(label_img)),\n",
        "                                lambda: (tr_img, label_img))\n",
        "  return tr_img, label_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDPWrM22K5A-"
      },
      "source": [
        "# Function to augment the data with vertical flip\n",
        "def flip_img_v(vertical_flip, tr_img, label_img):\n",
        "  if vertical_flip:\n",
        "    flip_prob = tf.random.uniform([], 0.0, 1.0)\n",
        "    tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n",
        "                                lambda: (tf.image.flip_up_down(tr_img), tf.image.flip_up_down(label_img)),\n",
        "                                lambda: (tr_img, label_img))\n",
        "  return tr_img, label_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg-4Yny6K5A-"
      },
      "source": [
        "# Function to augment the images and labels\n",
        "def _augment(img,\n",
        "             label_img,\n",
        "             resize=None,  # Resize the image to some size e.g. [256, 256]\n",
        "             scale=None,  # Scale image e.g. 1 / 255.\n",
        "             horizontal_flip=False,\n",
        "             vertical_flip=False): \n",
        "  if resize is not None:\n",
        "    # Resize both images\n",
        "    label_img = tf.image.resize(label_img, resize)\n",
        "    img = tf.image.resize(img, resize)\n",
        "  \n",
        "  img, label_img = flip_img_h(horizontal_flip, img, label_img)\n",
        "  img, label_img = flip_img_v(vertical_flip, img, label_img)\n",
        "  img = tf.cast(img, tf.float32) \n",
        "  if scale is not None:\n",
        "    img = tf.cast(img, tf.float32) * scale\n",
        "    #img = tf.keras.layers.Rescaling(scale=scale, offset=-1)\n",
        "  #label_img = tf.cast(label_img, tf.float32) * scale\n",
        "  #print(\"tensor: \", tf.unique(tf.keras.backend.print_tensor(label_img)))\n",
        "  return img, label_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSD5OyGMOkqh"
      },
      "source": [
        "# Main function to tie all of the above four dataset processing functions together \n",
        "def get_baseline_dataset(filenames, \n",
        "                         labels,\n",
        "                         preproc_fn=functools.partial(_augment),\n",
        "                         threads=5, \n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True):           \n",
        "  num_x = len(filenames)\n",
        "  # Create a dataset from the filenames and labels\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "  # Map our preprocessing function to every element in our dataset, taking\n",
        "  # advantage of multithreading\n",
        "  dataset = dataset.map(_process_pathnames, num_parallel_calls=threads)\n",
        "  if preproc_fn.keywords is not None and 'resize' not in preproc_fn.keywords:\n",
        "    assert batch_size == 1, \"Batching images must be of the same size\"\n",
        "\n",
        "  dataset = dataset.map(preproc_fn, num_parallel_calls=threads)\n",
        "  \n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(num_x)\n",
        "  \n",
        "  \n",
        "  # It's necessary to repeat our data for all epochs \n",
        "  dataset = dataset.repeat().batch(batch_size)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O8cw2FuOkqj"
      },
      "source": [
        "# dataset configuration for training\n",
        "tr_cfg = {\n",
        "    'resize': [img_shape[0], img_shape[1]],\n",
        "    'scale': 1 / 255.,\n",
        "    'horizontal_flip': True,\n",
        "    'vertical_flip': True,\n",
        "}\n",
        "tr_preprocessing_fn = functools.partial(_augment, **tr_cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyT9J6GtOkql"
      },
      "source": [
        "# dataset configuration for validation\n",
        "val_cfg = {\n",
        "    'resize': [img_shape[0], img_shape[1]],\n",
        "    'scale': 1 / 255.,\n",
        "}\n",
        "val_preprocessing_fn = functools.partial(_augment, **val_cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CStZ1BtZ6ZSi"
      },
      "source": [
        "# dataset configuration for testing\n",
        "test_cfg = {\n",
        "    'resize': [img_shape[0], img_shape[1]],\n",
        "    'scale': 1 / 255.,\n",
        "}\n",
        "test_preprocessing_fn = functools.partial(_augment, **test_cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn-tRHhPOkqm"
      },
      "source": [
        "# create the TensorFlow datasets\n",
        "train_ds = get_baseline_dataset(x_train_filenames,\n",
        "                                y_train_filenames,\n",
        "                                preproc_fn=tr_preprocessing_fn,\n",
        "                                batch_size=batch_size)\n",
        "val_ds = get_baseline_dataset(x_val_filenames,\n",
        "                              y_val_filenames, \n",
        "                              preproc_fn=val_preprocessing_fn,\n",
        "                              batch_size=batch_size)\n",
        "test_ds = get_baseline_dataset(x_test_filenames,\n",
        "                              y_test_filenames, \n",
        "                              preproc_fn=test_preprocessing_fn,\n",
        "                              batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OaIabcIOkqo"
      },
      "source": [
        "# Now we will display some samples from the datasets\n",
        "display_num = 1\n",
        "r_choices = np.random.choice(num_foreground_examples, 1)\n",
        "for i in range(0, display_num * 2, 2):\n",
        "  img_num = r_choices[i // 2]\n",
        "\n",
        "temp_ds = get_baseline_dataset(foreground_list_x[img_num:img_num+1], \n",
        "                               foreground_list_y[img_num:img_num+1],\n",
        "                               preproc_fn=tr_preprocessing_fn,\n",
        "                               batch_size=1,\n",
        "                               shuffle=False)\n",
        "\n",
        "# Let's examine some of these augmented images\n",
        "\n",
        "iterator = iter(temp_ds)\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "batch_of_imgs, label = next_element\n",
        "\n",
        "# Running next element in our graph will produce a batch of images\n",
        "\n",
        "sample_image, sample_mask = batch_of_imgs[0], label[0,:,:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI6l69EcOkqp"
      },
      "source": [
        "def display(display_list):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy6ptK5KOkqr"
      },
      "source": [
        "# display sample train image\n",
        "display([sample_image, sample_mask])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpWWK1DSOkqs"
      },
      "source": [
        "...same check for the validation images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVIrSUgoOkqu"
      },
      "source": [
        "# reset the forground list to capture the validation images\n",
        "foreground_list_x = []\n",
        "foreground_list_y = []\n",
        "for x,y in zip(x_val_filenames, y_val_filenames): \n",
        "    try:\n",
        "      filename_split = os.path.splitext(y) \n",
        "      filename_zero, fileext = filename_split \n",
        "      basename = os.path.basename(filename_zero) \n",
        "      if basename not in background_list_train:\n",
        "        foreground_list_x.append(x)\n",
        "        foreground_list_y.append(y)\n",
        "      else:\n",
        "        continue\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "num_foreground_examples = len(foreground_list_y)\n",
        "\n",
        "display_num = 1\n",
        "r_choices = np.random.choice(num_foreground_examples, 1)\n",
        "for i in range(0, display_num * 2, 2):\n",
        "  img_num = r_choices[i // 2]\n",
        "\n",
        "temp_ds = get_baseline_dataset(foreground_list_x[img_num:img_num+1], \n",
        "                               foreground_list_y[img_num:img_num+1],\n",
        "                               preproc_fn=val_preprocessing_fn,\n",
        "                               batch_size=1,\n",
        "                               shuffle=False)\n",
        "\n",
        "# Let's examine some of these augmented images\n",
        "\n",
        "iterator = iter(temp_ds)\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "batch_of_imgs, label = next_element\n",
        "\n",
        "# Running next element in our graph will produce a batch of images\n",
        "\n",
        "sample_image, sample_mask = batch_of_imgs[0], label[0,:,:,:]\n",
        "\n",
        "# display sample validation image\n",
        "display([sample_image, sample_mask])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mbFWy4i60qT"
      },
      "source": [
        "...same check for the test images:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWzKPy5268k-"
      },
      "source": [
        "# reset the forground list to capture the test images\n",
        "foreground_list_x = []\n",
        "foreground_list_y = []\n",
        "for x,y in zip(x_test_filenames, y_test_filenames): \n",
        "    try:\n",
        "      filename_split = os.path.splitext(y) \n",
        "      filename_zero, fileext = filename_split \n",
        "      basename = os.path.basename(filename_zero) \n",
        "      if basename not in background_list_train:\n",
        "        foreground_list_x.append(x)\n",
        "        foreground_list_y.append(y)\n",
        "      else:\n",
        "        continue\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "num_foreground_examples = len(foreground_list_y)\n",
        "\n",
        "display_num = 1\n",
        "r_choices = np.random.choice(num_foreground_examples, 1)\n",
        "for i in range(0, display_num * 2, 2):\n",
        "  img_num = r_choices[i // 2]\n",
        "\n",
        "temp_ds = get_baseline_dataset(foreground_list_x[img_num:img_num+1], \n",
        "                               foreground_list_y[img_num:img_num+1],\n",
        "                               preproc_fn=test_preprocessing_fn,\n",
        "                               batch_size=1,\n",
        "                               shuffle=False)\n",
        "\n",
        "# Let's examine some of these augmented images\n",
        "\n",
        "iterator = iter(temp_ds)\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "batch_of_imgs, label = next_element\n",
        "\n",
        "# Running next element in our graph will produce a batch of images\n",
        "\n",
        "sample_image, sample_mask = batch_of_imgs[0], label[0,:,:,:]\n",
        "\n",
        "# display sample test image\n",
        "display([sample_image, sample_mask])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbypQtG9Okqy"
      },
      "source": [
        "### Define the model\n",
        "\n",
        "The model being used here is a modified U-Net. A U-Net consists of an encoder (downsampler) and decoder (upsampler). In-order to learn robust features, and reduce the number of trainable parameters, an optionally pretrained model can be used as the encoder. Thus, the encoder for this task will be a pretrained MobileNetV2 model, whose intermediate outputs will be used, and the decoder will be the upsample block already implemented in TensorFlow Examples in the Pix2pix tutorial.\n",
        "\n",
        ":::{figure-md} Unet_mobilenetv2_arch_arch-fig\n",
        "\n",
        "<img src=\"images/Unet_mobilenetv2_arch_arch.png\" width=\"650px\">\n",
        "\n",
        "U-shaped MobileNetV2 (adapted U-Net) architecture diagram (from [Sarakon et al., 2019](https://www.researchgate.net/publication/339266308_Surface-Defect_Segmentation_using_U-shaped_Inverted_Residuals)). \n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIkwQ4yqkuc5"
      },
      "source": [
        "The reason to output nine channels is because there are nine possible labels for each pixel. Think of this as multi-classification where each pixel is being classified into nine classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eCVw15_Okqy"
      },
      "source": [
        "# set number of model output channels to the number of classes (including background)\n",
        "OUTPUT_CHANNELS = 9 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOjXoO6bOkq0"
      },
      "source": [
        "As mentioned, the encoder will be an optionally pretrained MobileNetV2 model which is prepared and ready to use in tf.keras.applications. The encoder consists of specific outputs from intermediate layers in the model. Note that the encoder will be trained during the training process, as it would necessitate optical imagery if transfer learning were to be used, and in this example we are using spectral indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zizw1IhjOkq1"
      },
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[224, 224, 3], include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',\n",
        "    'block_3_expand_relu',\n",
        "    'block_6_expand_relu', \n",
        "    'block_13_expand_relu',\n",
        "    'block_16_project', \n",
        "]\n",
        "layers = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
        "\n",
        "down_stack.trainable = True # Set this to False if using pre-trained weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnBYDnfaOkq3"
      },
      "source": [
        "The decoder/upsampler is simply a series of upsample blocks implemented in TensorFlow examples. The arguments are (number of filters, kernel size). A kernel size of 3 is considered standard for current network implementations ([Sandler et al., 2018](https://arxiv.org/abs/1801.04381)). You can increase or decrease the number of filters, however, more filters equates to more parameters and longer training time. A range between 32 to 512, increasing 2x for each successive convolutional layer and decreasing for each successive deconvolutional/upsampling layer, is very common across different convolutional network architectures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kgm994UAOkq3"
      },
      "source": [
        "up_stack = [\n",
        "    pix2pix.upsample(512, 3),\n",
        "    pix2pix.upsample(256, 3),\n",
        "    pix2pix.upsample(128, 3),\n",
        "    pix2pix.upsample(64, 3),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kZgk-UdOkq5"
      },
      "source": [
        "def unet_model(output_channels):\n",
        "  inputs = tf.keras.layers.Input(shape=[224,224,3], name='first_layer')\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = down_stack(x)\n",
        "  x = skips[-1]\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    concat = tf.keras.layers.Concatenate()\n",
        "    x = concat([x, skip])\n",
        "\n",
        "  # This is the last layer of the model\n",
        "  last = tf.keras.layers.Conv2DTranspose(\n",
        "      output_channels, 3, strides=2,\n",
        "      padding='same', name='last_layer')\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebnPi5m0Okq6"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Now, all that is left to do is to compile and train the model. The loss being used here is SparseCategoricalFocalLoss(from_logits=True). The reason to use this loss function is 1) because the network is trying to assign each pixel a label, just like multi-class prediction, and 2) because focal loss weights the relative contribution of each class by the distribution in the dataset to emphasize under-represented classes and dampen over-represented classes. In the true segmentation mask, each pixel has a value between 0-9. The network here is outputting ten channels. Essentially, each channel is trying to learn to predict a class, and SparseCategoricalFocalLoss(from_logits=True) is the recommended loss for such a scenario. Using the output of the network, the label assigned to the pixel is the channel with the highest value. This is what the create_mask function is doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvMU895pOkq7"
      },
      "source": [
        "model = unet_model(OUTPUT_CHANNELS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__UhFBnXIMlc"
      },
      "source": [
        "Check the network layer output shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU94s96CGpwg"
      },
      "source": [
        "for layer in model.layers:\n",
        "    print(layer.name, layer.output_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws87uKATOkrB"
      },
      "source": [
        "Find the class weights for the focal loss function to help address class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM7UVWNwpvvH"
      },
      "source": [
        "train_df_para = gpd.read_file('TerraBio_Para.geojson')\n",
        "train_df_imaflora = gpd.read_file('TerraBio_Imaflora.geojson')\n",
        "train_df = pd.concat([train_df_para, train_df_imaflora])\n",
        "inv_freq = np.array(1/(train_df.landcover.value_counts()/len(train_df)))\n",
        "inv_freq = [0.,*inv_freq]\n",
        "class_weights = {0 : inv_freq[0], 1: inv_freq[1], 2: inv_freq[2], 3: inv_freq[3], \n",
        "                4: inv_freq[4], 5: inv_freq[5], 6: inv_freq[6],\n",
        "                7: inv_freq[7], 8: inv_freq[8]}\n",
        "\n",
        "def NormalizeData(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "class_weights_list = list(class_weights.values())\n",
        "print(\"class weights: \", class_weights_list)\n",
        "scaled_class_weights = NormalizeData(class_weights_list)\n",
        "scaled_class_weights_list = scaled_class_weights.tolist() \n",
        "print(\"scaled class weights: \", scaled_class_weights_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLD0gOSfkuc6"
      },
      "source": [
        "In the SparseCategoricalFocalLoss function, gamma is the focusing parameter. Higher values of gamma make dominant, or \"easy to classify\", examples contribute less to the loss relative to rare, or \"difficult to classify\", examples. The value for gamma must be non-negative, and the authors of this loss function found that empirically a gamma value of 2 works best ([Lin et al., 2017](https://arxiv.org/abs/1708.02002)). You can experiment by adding the class weights as a parameter to SparseCategoricalFocalLoss, e.g. `SparseCategoricalFocalLoss(gamma=2, class_weight=scaled_class_weights_list, from_logits=True)`.\n",
        "\n",
        "We will measure our model's performance during training by per-pixel accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8441AatOkrB"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
        "              loss=SparseCategoricalFocalLoss(gamma=2, from_logits=True), #tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy', iou_score])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXcrmls1OkrG"
      },
      "source": [
        "Let's try out the un/pre-trained model to see what it predicts before training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ6I0-hSOkrG"
      },
      "source": [
        "def create_mask(pred_mask):\n",
        "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., tf.newaxis]\n",
        "  return pred_mask[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MuvmEVdOkrI"
      },
      "source": [
        "def show_predictions(image=None, mask=None, dataset=None, num=1):\n",
        "  if image is None and dataset is None:\n",
        "    # this is just for showing keras callback output. in practice this should be broken out into a different function\n",
        "    sample_image = skio.imread(f'{img_dir}/tile_terrabio_17507.png') * (1/255.)\n",
        "    sample_mask = skio.imread(f'{label_dir}/tile_terrabio_17507.png')\n",
        "    mp = create_mask(model.predict(sample_image[tf.newaxis, ...]))\n",
        "    mpe = tf.keras.backend.eval(mp)\n",
        "    display([sample_image, sample_mask[..., tf.newaxis], mpe])\n",
        "  elif dataset:\n",
        "    for image, mask in dataset.take(num):\n",
        "      pred_mask = model.predict(image)\n",
        "      display([image[0], mask[0], create_mask(pred_mask)])\n",
        "  else:\n",
        "    mp = create_mask(model.predict(image[tf.newaxis, ...]))\n",
        "    mpe = tf.keras.backend.eval(mp)\n",
        "    display([image, mask, mpe])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g43jR4WIOkrJ"
      },
      "source": [
        "show_predictions(image=sample_image, mask=sample_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMWgngxaOkrO"
      },
      "source": [
        "Let's observe how the model improves while it is training. To accomplish this task, a callback function is defined below to plot a test image and its predicted mask after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcmPc9q9OkrO"
      },
      "source": [
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    clear_output(wait=True)\n",
        "    show_predictions()\n",
        "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3pybdpIK5BC"
      },
      "source": [
        "We may want to view the model graph and training progress in TensorBoard, so we'll establish a callback to save logs to a dedicated directory which will serve the TensorBoard interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxifHWC3K5BC"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFlZsrDuK5BD"
      },
      "source": [
        "log_dir = os.path.join(workshop_dir,'logs/')\n",
        "log_fit_dir = os.path.join(workshop_dir,'logs', 'fit')\n",
        "log_fit_session_dir = os.path.join(workshop_dir,'logs', 'fit', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "visualizations_dir = os.path.join(workshop_dir,'logs', 'vizualizations')\n",
        "visualizations_session_dir = os.path.join(workshop_dir,'logs', 'vizualizations', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "dirs = [log_fit_dir, visualizations_dir]\n",
        "for dir in dirs:\n",
        "  if (os.path.isdir(dir)):\n",
        "    print(\"Making fresh log dir.\")\n",
        "    shutil.rmtree(dir)\n",
        "  else:\n",
        "    print(\"Fresh log dir exists.\")\n",
        "\n",
        "dirs = [log_dir, log_fit_dir, log_fit_session_dir, visualizations_dir, visualizations_session_dir]\n",
        "for dir in dirs:\n",
        "  if (not os.path.isdir(dir)):\n",
        "    os.mkdir(dir)\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_fit_session_dir, histogram_freq=1, write_graph=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5DhL43UK5BD"
      },
      "source": [
        "We can also take a look at the layer activations in TensorBoard using tf-explain, so we'll establish a callback to save activation visualizations to a dedicated directory which will serve the TensorBoard interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka5brbF-K5BD"
      },
      "source": [
        "# get a batch of validation samples to plot activations for\n",
        "for example in val_ds.take(1):\n",
        "  image_val, label_val = example[0], example[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5_-BgrhK5BD"
      },
      "source": [
        "callbacks = [\n",
        "    ActivationsVisualizationCallback(\n",
        "        validation_data=(image_val, label_val),\n",
        "        layers_name=[\"last_layer\"], \n",
        "        output_dir=visualizations_session_dir,\n",
        "    ),\n",
        "    DisplayCallback(),\n",
        "    tensorboard_callback\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQy7CbaUOkrQ"
      },
      "source": [
        "#### Fit and View\n",
        "Now we will actually train the model for 4 epochs (full cycles through the training dataset), visualizing predictions on a validation image after each epoch. In practice, you would want to train the model until validation loss starts to increase (a clear indication of overfitting). Empirically with this dataset, convergence occurred around 50 epochs. We've reduced to 4 epochs purely for rapid demonstration purposes. As a preview, at 50 epochs you should observe a test prediction similar to:\n",
        "\n",
        "![testimage](images/epoch50_testimage.png)\n",
        "![testimage1](images/epoch50_testimage1.png)\n",
        "\n",
        "Don't be alarmed if you see blank predictions after only 4 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLL4CJ-dOkrQ"
      },
      "source": [
        "EPOCHS = 4\n",
        "\n",
        "model_history = model.fit(train_ds, \n",
        "                   steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))),\n",
        "                   epochs=EPOCHS,\n",
        "                   validation_data=val_ds,\n",
        "                   validation_steps=int(np.ceil(num_val_examples / float(batch_size))),\n",
        "                   callbacks=callbacks) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GKAdqfhOkrS"
      },
      "source": [
        "Plot the model's learning curve over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYyXxafUOkrS"
      },
      "source": [
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "\n",
        "epochs = range(EPOCHS)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'bo', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47PnYd2pK5BD"
      },
      "source": [
        "#### Start TensorBoard \n",
        "\n",
        "You set the `logdir` in the below command to `..logs/visualizations` to see the activations or `..logs/fit` to see the model scalars, graphs, distributions and histograms (described below).\n",
        "\n",
        "The dashboards can be selected from the tabs in top navigation bar.\n",
        "\n",
        "1. The Scalars dashboard shows how the loss and metrics change with every  epoch. You can use it to also track training speed, learning rate, and other scalar values.\n",
        "2. The Graphs dashboard helps you visualize your model. In this case, the Keras graph of layers is shown which can help you ensure it is built correctly.\n",
        "3. The Distributions and Histograms dashboards show the distribution of a Tensor over time. This can be useful to visualize weights and biases and verify that they are changing in an expected way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuJb7nM5AST4"
      },
      "source": [
        "%tensorboard --logdir \"$visualizations_dir\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNfMRajUOkrT"
      },
      "source": [
        "---------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpfo17mvOkrf"
      },
      "source": [
        "#### Save model to file\n",
        "\n",
        "We will export the final model weights to your own google drive folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JwD9-FqOkrg"
      },
      "source": [
        "if (not os.path.isdir(workshop_dir)):\n",
        "  os.mkdir(workshop_dir)\n",
        "save_model_path = os.path.join(workshop_dir,'model_out_batch_{}_ep{}_nopretrain_focalloss/'.format(batch_size, EPOCHS))\n",
        "if (not os.path.isdir(save_model_path)):\n",
        "  os.mkdir(save_model_path)\n",
        "model.save(save_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNmZD3cXOkrT"
      },
      "source": [
        "### Make predictions\n",
        "\n",
        "Let's make some predictions. In the interest of saving time, the number of epochs was kept small, but you may set this higher to achieve more accurate results. We'll load from the rea donly workshop directory in case you weren't able to save your own model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8XKX6PTNT7t"
      },
      "source": [
        "# Optional, you can load the model from the saved version\n",
        "load_from_checkpoint = True\n",
        "if load_from_checkpoint == True:\n",
        "  save_model_path = os.path.join(workshop_dir,'model_out_batch_{}_ep{}_nopretrain_focalloss/'.format(batch_size, EPOCHS))\n",
        "  model = tf.keras.models.load_model(save_model_path, custom_objects={\"loss\": SparseCategoricalFocalLoss, \"iou_score\": iou_score})\n",
        "else:\n",
        "  print(\"inferencing from in memory model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyLc1EK_OkrU"
      },
      "source": [
        "def get_predictions(image= None, dataset=None, num=1):\n",
        "  if image is None and dataset is None:\n",
        "    return ValueError(\"At least one of image or dataset must not be None.\")\n",
        "  if dataset:\n",
        "    for image, mask in dataset.take(num):\n",
        "      pred_mask = model.predict(image)\n",
        "      return pred_mask\n",
        "  else:\n",
        "    pred_mask = create_mask(model.predict(image[tf.newaxis, ...]))\n",
        "    pred_mask = tf.keras.backend.eval(pred_mask)\n",
        "    return pred_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sHutTlADYMM"
      },
      "source": [
        "#### Single image example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt0YJ7qDOkrW"
      },
      "source": [
        "display_num = 1\n",
        "r_choices = np.random.choice(num_foreground_examples, 1)\n",
        "for i in range(0, display_num * 2, 2):\n",
        "  img_num = r_choices[i // 2]\n",
        "\n",
        "temp_ds = get_baseline_dataset(foreground_list_x[img_num:img_num+1], \n",
        "                               foreground_list_y[img_num:img_num+1],\n",
        "                               preproc_fn=test_preprocessing_fn,\n",
        "                               batch_size=1,\n",
        "                               shuffle=False)\n",
        "\n",
        "# Let's examine some of these augmented images\n",
        "\n",
        "iterator = iter(temp_ds)\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "batch_of_imgs, label = next_element\n",
        "\n",
        "# Running next element in our graph will produce a batch of images\n",
        "\n",
        "sample_image, sample_mask = batch_of_imgs[0], label[0,:,:,:]\n",
        "\n",
        "# run and plot predicitions\n",
        "pred_mask = get_predictions(sample_image)\n",
        "\n",
        "show_predictions(image=sample_image, mask=sample_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rncgzkleDdKr"
      },
      "source": [
        "#### Multi image example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_p8tsAvOkrZ",
        "scrolled": true
      },
      "source": [
        "tiled_prediction_dir = os.path.join(workshop_dir,'predictions_test_focal_loss/')\n",
        "if not os.path.exists(tiled_prediction_dir):\n",
        "    os.makedirs(tiled_prediction_dir)\n",
        "    \n",
        "pred_masks = []\n",
        "pred_paths = []\n",
        "true_masks = []\n",
        "\n",
        "for i in range(0, len(x_test_filenames)):\n",
        "    img_num = i\n",
        "\n",
        "    try:\n",
        "      temp_ds = get_baseline_dataset(x_test_filenames[img_num:img_num+1], \n",
        "                                   y_test_filenames[img_num:img_num+1],\n",
        "                                   preproc_fn=test_preprocessing_fn,\n",
        "                                   batch_size=1,\n",
        "                                   shuffle=False)\n",
        "    except Exception as e: \n",
        "      print(str(e))\n",
        "\n",
        "    # Let's examine some of these augmented images\n",
        "\n",
        "    iterator = iter(temp_ds)\n",
        "    next_element = iterator.get_next()\n",
        "\n",
        "    batch_of_imgs, label = next_element\n",
        "\n",
        "    # Running next element in our graph will produce a batch of images\n",
        "    image, mask = batch_of_imgs[0], label[0,:,:,:]\n",
        "    mask_int = tf.dtypes.cast(mask, tf.int32)\n",
        "    true_masks.append(mask_int)\n",
        "    print(y_test_filenames[img_num:img_num+1])\n",
        "    print(np.unique(mask_int))\n",
        "\n",
        "    # run and plot predicitions, only showing every 27th prediction\n",
        "    if img_num % 27 == 0:\n",
        "        show_predictions(image=image, mask=mask)\n",
        "    pred_mask = get_predictions(image)\n",
        "    pred_masks.append(pred_mask)\n",
        "    \n",
        "    # save prediction images to file\n",
        "\n",
        "    filename_split = os.path.splitext(x_test_filenames[img_num]) \n",
        "    filename_zero, fileext = filename_split \n",
        "    basename = os.path.basename(filename_zero) \n",
        "    pred_path = os.path.join(tiled_prediction_dir, \"{}.png\".format(basename))\n",
        "    pred_paths.append(pred_path)\n",
        "    tf.keras.preprocessing.image.save_img(pred_path,pred_mask, scale=False) # scaling is good to do to cut down on file size, but adds an extra dtype conversion step.    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzkQfM72K5BF"
      },
      "source": [
        "Finally, we will save a csv with our test file paths so we can easily load predictions and labels in the next lesson to calculate our evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_OQXvkqK5BF"
      },
      "source": [
        "path_df = pd.DataFrame(list(zip(x_test_filenames, y_test_filenames, pred_paths)), columns=[\"img_names\", \"label_names\", \"pred_names\"])\n",
        "path_df.to_csv(os.path.join(workshop_dir, \"test_file_paths.csv\"))\n",
        "\n",
        "path_df = pd.DataFrame(list(zip(x_train_filenames, y_train_filenames)), columns=[\"img_names\", \"label_names\"])\n",
        "path_df.to_csv(os.path.join(workshop_dir, \"train_file_paths.csv\"))\n",
        "\n",
        "path_df = pd.DataFrame(list(zip(x_val_filenames, y_val_filenames)), columns=[\"img_names\", \"label_names\"])\n",
        "path_df.to_csv(os.path.join(workshop_dir, \"validate_file_paths.csv\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZawSD0lGK5BF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}